{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f30620c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c44628ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL DEFINITION ===\n",
    "def create_cnn_model(in_channels=3, conv_filters=[32, 64], kernel_size=3, fc_units=[128], \n",
    "                     dropout=0.5, num_classes=3, flatten_type=\"flatten\"):\n",
    "    layers = []\n",
    "    input_c = in_channels\n",
    "\n",
    "    for out_c in conv_filters:\n",
    "        layers.append(nn.Conv2d(input_c, out_c, kernel_size, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.MaxPool2d(2))\n",
    "        input_c = out_c\n",
    "\n",
    "    if flatten_type == \"gap\":\n",
    "        layers.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
    "        layers.append(nn.Flatten())\n",
    "        num_flat_features = conv_filters[-1] * 1 * 1\n",
    "    else:\n",
    "        layers.append(nn.AdaptiveAvgPool2d((4, 4)))\n",
    "        layers.append(nn.Flatten())\n",
    "        num_flat_features = conv_filters[-1] * 4 * 4\n",
    "\n",
    "    for units in fc_units:\n",
    "        layers.append(nn.Linear(num_flat_features, units))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "        num_flat_features = units\n",
    "\n",
    "    layers.append(nn.Linear(num_flat_features, num_classes))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70b8802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = [\n",
    "    {\n",
    "        \"in_channels\": 3,\n",
    "        \"conv_filters\": [32, 64],\n",
    "        \"fc_units\": [256],\n",
    "        \"dropout\": 0.5,\n",
    "        \"flatten_type\": \"flatten\",\n",
    "        \"num_classes\": 3\n",
    "    },\n",
    "    {\n",
    "        \"in_channels\": 3,\n",
    "        \"conv_filters\": [64, 128],\n",
    "        \"fc_units\": [512, 128],\n",
    "        \"dropout\": 0.4,\n",
    "        \"flatten_type\": \"gap\",\n",
    "        \"num_classes\": 3\n",
    "    },\n",
    "    {\n",
    "        \"in_channels\": 3,\n",
    "        \"conv_filters\": [32, 64, 128],\n",
    "        \"fc_units\": [1024, 512, 128],\n",
    "        \"dropout\": 0.3,\n",
    "        \"flatten_type\": \"flatten\",\n",
    "        \"num_classes\": 3\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d23ac8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_function(loss_name='bcewithlogits'):\n",
    "    if loss_name == 'bcewithlogits':\n",
    "        return nn.BCEWithLogitsLoss()\n",
    "    elif loss_name == 'bceloss':\n",
    "        return nn.BCELoss()\n",
    "    elif loss_name == 'mse':\n",
    "        return nn.MSELoss()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss function: {loss_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be2580be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(optimizer_name, model_parameters, lr=1e-3, weight_decay=0):\n",
    "    if optimizer_name.lower() == 'adam':\n",
    "        return optim.Adam(model_parameters, lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_name.lower() == 'sgd':\n",
    "        return optim.SGD(model_parameters, lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    elif optimizer_name.lower() == 'rmsprop':\n",
    "        return optim.RMSprop(model_parameters, lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39d2cb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahhal AbuZahra\\AppData\\Local\\Temp\\ipykernel_19696\\2242991132.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_tensors = torch.load('train_tensors.pt')\n",
      "C:\\Users\\Rahhal AbuZahra\\AppData\\Local\\Temp\\ipykernel_19696\\2242991132.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_tensors = torch.load('val_tensors.pt')\n",
      "C:\\Users\\Rahhal AbuZahra\\AppData\\Local\\Temp\\ipykernel_19696\\2242991132.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_tensors = torch.load('test_tensors.pt')\n"
     ]
    }
   ],
   "source": [
    "# Load tensors\n",
    "train_tensors = torch.load('train_tensors.pt')\n",
    "val_tensors = torch.load('val_tensors.pt')\n",
    "test_tensors = torch.load('test_tensors.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bd7c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(*train_tensors)\n",
    "val_dataset = TensorDataset(*val_tensors)\n",
    "test_dataset = TensorDataset(*test_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b135e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)  # adjust batch_size if needed\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06ba018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "    best_val_acc = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.numel()\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                probs = torch.sigmoid(outputs)\n",
    "                preds = (probs > 0.5).float()\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.numel()\n",
    "\n",
    "                all_preds.append(preds.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "\n",
    "        all_preds = torch.cat(all_preds).numpy()\n",
    "        all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "        val_acc = val_correct / val_total\n",
    "        val_loss /= val_total\n",
    "\n",
    "        precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "        mlflow.log_metrics({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "            \"val_precision\": precision,\n",
    "            \"val_recall\": recall,\n",
    "            \"val_f1\": f1\n",
    "        }, step=epoch)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "    return model, best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73b54f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b19430c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/18 23:02:32 INFO mlflow.tracking.fluent: Experiment with name 'celebA_models' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///d:/4th%20year/2nd%20semester/Training%203/Final%20Project/mlruns/555883558464305941', creation_time=1747598552165, experiment_id='555883558464305941', last_update_time=1747598552165, lifecycle_stage='active', name='celebA_models', tags={}>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"celebA_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbe7885c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|██████████| 109/109 [01:54<00:00,  1.05s/it]\n",
      "Epoch 1/10 [Val]: 100%|██████████| 79/79 [00:43<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Acc: 0.6473, Val Acc: 0.7086, F1: 0.5802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 109/109 [01:47<00:00,  1.01it/s]\n",
      "Epoch 2/10 [Val]: 100%|██████████| 79/79 [00:39<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Acc: 0.7185, Val Acc: 0.7769, F1: 0.6237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|██████████| 109/109 [01:36<00:00,  1.13it/s]\n",
      "Epoch 3/10 [Val]: 100%|██████████| 79/79 [00:39<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Acc: 0.7481, Val Acc: 0.7991, F1: 0.6494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|██████████| 109/109 [01:37<00:00,  1.12it/s]\n",
      "Epoch 4/10 [Val]: 100%|██████████| 79/79 [00:39<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Acc: 0.7631, Val Acc: 0.8139, F1: 0.6767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]: 100%|██████████| 109/109 [01:35<00:00,  1.14it/s]\n",
      "Epoch 5/10 [Val]: 100%|██████████| 79/79 [00:39<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Acc: 0.7730, Val Acc: 0.7974, F1: 0.6757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]: 100%|██████████| 109/109 [01:38<00:00,  1.11it/s]\n",
      "Epoch 6/10 [Val]: 100%|██████████| 79/79 [00:40<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train Acc: 0.7827, Val Acc: 0.8134, F1: 0.6646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]: 100%|██████████| 109/109 [01:37<00:00,  1.12it/s]\n",
      "Epoch 7/10 [Val]: 100%|██████████| 79/79 [00:39<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train Acc: 0.7956, Val Acc: 0.8297, F1: 0.6935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]: 100%|██████████| 109/109 [01:37<00:00,  1.12it/s]\n",
      "Epoch 8/10 [Val]: 100%|██████████| 79/79 [00:40<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train Acc: 0.8014, Val Acc: 0.8367, F1: 0.7032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]: 100%|██████████| 109/109 [01:35<00:00,  1.15it/s]\n",
      "Epoch 9/10 [Val]: 100%|██████████| 79/79 [00:39<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train Acc: 0.8119, Val Acc: 0.8339, F1: 0.7042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]: 100%|██████████| 109/109 [01:41<00:00,  1.07it/s]\n",
      "Epoch 10/10 [Val]: 100%|██████████| 79/79 [00:35<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train Acc: 0.8186, Val Acc: 0.8410, F1: 0.7202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|██████████| 109/109 [05:14<00:00,  2.88s/it]\n",
      "Epoch 1/10 [Val]: 100%|██████████| 79/79 [02:27<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Acc: 0.5487, Val Acc: 0.6647, F1: 0.4102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 109/109 [04:11<00:00,  2.31s/it]\n",
      "Epoch 2/10 [Val]: 100%|██████████| 79/79 [01:37<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Acc: 0.5842, Val Acc: 0.6524, F1: 0.4289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|██████████| 109/109 [04:02<00:00,  2.22s/it]\n",
      "Epoch 3/10 [Val]: 100%|██████████| 79/79 [01:33<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Acc: 0.6037, Val Acc: 0.6411, F1: 0.4745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|██████████| 109/109 [04:02<00:00,  2.23s/it]\n",
      "Epoch 4/10 [Val]: 100%|██████████| 79/79 [01:35<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Acc: 0.6154, Val Acc: 0.6623, F1: 0.4325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]: 100%|██████████| 109/109 [04:00<00:00,  2.21s/it]\n",
      "Epoch 5/10 [Val]: 100%|██████████| 79/79 [01:33<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Acc: 0.6293, Val Acc: 0.6695, F1: 0.4825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]: 100%|██████████| 109/109 [04:00<00:00,  2.21s/it]\n",
      "Epoch 6/10 [Val]: 100%|██████████| 79/79 [01:33<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train Acc: 0.6423, Val Acc: 0.6764, F1: 0.3912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]: 100%|██████████| 109/109 [04:01<00:00,  2.22s/it]\n",
      "Epoch 7/10 [Val]: 100%|██████████| 79/79 [01:33<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train Acc: 0.6484, Val Acc: 0.6997, F1: 0.4874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]: 100%|██████████| 109/109 [04:00<00:00,  2.20s/it]\n",
      "Epoch 8/10 [Val]: 100%|██████████| 79/79 [01:32<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train Acc: 0.6626, Val Acc: 0.6927, F1: 0.5328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]: 100%|██████████| 109/109 [03:59<00:00,  2.19s/it]\n",
      "Epoch 9/10 [Val]: 100%|██████████| 79/79 [01:33<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train Acc: 0.6689, Val Acc: 0.6518, F1: 0.5237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]: 100%|██████████| 109/109 [03:58<00:00,  2.18s/it]\n",
      "Epoch 10/10 [Val]: 100%|██████████| 79/79 [01:33<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train Acc: 0.6751, Val Acc: 0.7048, F1: 0.5525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|██████████| 109/109 [02:12<00:00,  1.21s/it]\n",
      "Epoch 1/10 [Val]: 100%|██████████| 79/79 [00:50<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Acc: 0.6276, Val Acc: 0.7518, F1: 0.6279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 109/109 [02:11<00:00,  1.21s/it]\n",
      "Epoch 2/10 [Val]: 100%|██████████| 79/79 [00:50<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Acc: 0.7198, Val Acc: 0.7783, F1: 0.6609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|██████████| 109/109 [02:13<00:00,  1.22s/it]\n",
      "Epoch 3/10 [Val]: 100%|██████████| 79/79 [00:50<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Acc: 0.7540, Val Acc: 0.8130, F1: 0.6614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|██████████| 109/109 [02:16<00:00,  1.25s/it]\n",
      "Epoch 4/10 [Val]: 100%|██████████| 79/79 [00:50<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Acc: 0.7855, Val Acc: 0.8340, F1: 0.7154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]: 100%|██████████| 109/109 [02:14<00:00,  1.23s/it]\n",
      "Epoch 5/10 [Val]: 100%|██████████| 79/79 [00:51<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Acc: 0.8160, Val Acc: 0.8627, F1: 0.7648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]: 100%|██████████| 109/109 [02:12<00:00,  1.21s/it]\n",
      "Epoch 6/10 [Val]: 100%|██████████| 79/79 [00:51<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train Acc: 0.8356, Val Acc: 0.8720, F1: 0.7629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]: 100%|██████████| 109/109 [02:16<00:00,  1.25s/it]\n",
      "Epoch 7/10 [Val]: 100%|██████████| 79/79 [00:51<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train Acc: 0.8601, Val Acc: 0.8829, F1: 0.7673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]: 100%|██████████| 109/109 [02:13<00:00,  1.23s/it]\n",
      "Epoch 8/10 [Val]: 100%|██████████| 79/79 [00:50<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train Acc: 0.8795, Val Acc: 0.8928, F1: 0.7717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]: 100%|██████████| 109/109 [02:12<00:00,  1.21s/it]\n",
      "Epoch 9/10 [Val]: 100%|██████████| 79/79 [00:49<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train Acc: 0.8952, Val Acc: 0.9071, F1: 0.8067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]: 100%|██████████| 109/109 [02:12<00:00,  1.22s/it]\n",
      "Epoch 10/10 [Val]: 100%|██████████| 79/79 [00:50<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train Acc: 0.9050, Val Acc: 0.9183, F1: 0.8285\n"
     ]
    }
   ],
   "source": [
    "for i, config in enumerate(model_configs):\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"celeba_model_{i+1}\"):\n",
    "        config[\"in_channels\"] = 3\n",
    "        \n",
    "        mlflow.log_params(config)\n",
    "\n",
    "        model = create_cnn_model(**config)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        trained_model, best_val_acc = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device=device)\n",
    "\n",
    "        mlflow.log_metric(\"best_val_acc\", best_val_acc)\n",
    "\n",
    "        # Save model artifact\n",
    "        model_path = f\"model_{i+1}.pt\"\n",
    "        torch.save(trained_model.state_dict(), model_path)\n",
    "        mlflow.log_artifact(model_path)\n",
    "        os.remove(model_path)\n",
    "\n",
    "        mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac76a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41d16f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create ResNet Model \n",
    "def create_resnet_for_multilabel(num_labels=3, freeze_features=False):\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(in_features, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(256, num_labels),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    if freeze_features:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ffcd965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Evaluation Function \n",
    "def evaluate_model(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in data_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device).float()\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "            preds = (outputs > 0.5).float()\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.numel()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    accuracy = total_correct / total_samples\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9a1e8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Training Loop \n",
    "def train_model(model, train_loader, val_loader, loss_fn, optimizer, num_epochs, device):\n",
    "    \n",
    "    with mlflow.start_run(run_name='resnet_model_run'):\n",
    "        model.to(device)\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for imgs, labels in train_loader:\n",
    "                imgs, labels = imgs.to(device), labels.to(device).float()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(imgs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss, val_acc = evaluate_model(model, val_loader, loss_fn, device)\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            mlflow.log_metric(\"train_loss\", epoch_loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_accuracy\", val_acc, step=epoch)\n",
    "\n",
    "        # Log model and parameters\n",
    "        mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "        mlflow.log_param(\"optimizer\", type(optimizer).__name__)\n",
    "        mlflow.log_param(\"learning_rate\", optimizer.param_groups[0]['lr'])\n",
    "        mlflow.pytorch.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc9674a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rahhal AbuZahra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rahhal AbuZahra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Loss: 0.5842 | Val Loss: 0.6023 | Val Acc: 0.9374\n",
      "Epoch [2/10] Loss: 0.5301 | Val Loss: 0.5979 | Val Acc: 0.9395\n",
      "Epoch [3/10] Loss: 0.5204 | Val Loss: 0.5972 | Val Acc: 0.9481\n",
      "Epoch [4/10] Loss: 0.5155 | Val Loss: 0.5972 | Val Acc: 0.9499\n",
      "Epoch [5/10] Loss: 0.5128 | Val Loss: 0.5982 | Val Acc: 0.9520\n",
      "Epoch [6/10] Loss: 0.5111 | Val Loss: 0.5953 | Val Acc: 0.9515\n",
      "Epoch [7/10] Loss: 0.5102 | Val Loss: 0.6010 | Val Acc: 0.9488\n",
      "Epoch [8/10] Loss: 0.5094 | Val Loss: 0.5938 | Val Acc: 0.9531\n",
      "Epoch [9/10] Loss: 0.5081 | Val Loss: 0.5946 | Val Acc: 0.9541\n",
      "Epoch [10/10] Loss: 0.5081 | Val Loss: 0.5934 | Val Acc: 0.9545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/19 02:12:18 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "model = create_resnet_for_multilabel(num_labels=3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "train_model(model , train_loader , val_loader , loss_fn , optimizer , 10 , device)\n",
    "\n",
    "with open(\"resnet_model.pkl\", \"wb\") as f:\n",
    "    torch.save(model, f)\n",
    "\n",
    "torch.save(model.state_dict(), \"resnet_model_weights.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5da86cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "def evaluate_model_on_test(model, test_loader, threshold=0.5, device='cuda'):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.cpu().numpy()\n",
    "            outputs = model(imgs).cpu().numpy()\n",
    "            preds = (outputs > threshold).astype(int)\n",
    "\n",
    "            all_labels.extend(labels)\n",
    "            all_preds.extend(preds)\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "\n",
    "    accuracy = (all_labels == all_preds).mean()\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    print(\"==== Test Evaluation ====\")\n",
    "    print(f\"Exact Match Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Macro F1 Score:       {f1:.4f}\")\n",
    "    print(f\"Macro Precision:      {precision:.4f}\")\n",
    "    print(f\"Macro Recall:         {recall:.4f}\")\n",
    "\n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cd43bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Test Evaluation ====\n",
      "Exact Match Accuracy: 0.9522\n",
      "Macro F1 Score:       0.8951\n",
      "Macro Precision:      0.8815\n",
      "Macro Recall:         0.9205\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test set\n",
    "accuracy, f1, precision, recall = evaluate_model_on_test(model, test_loader, threshold=0.5, device=device)\n",
    "\n",
    "# log to MLflow\n",
    "mlflow.log_metric(\"test_accuracy\", accuracy)\n",
    "mlflow.log_metric(\"test_f1\", f1)\n",
    "mlflow.log_metric(\"test_precision\", precision)\n",
    "mlflow.log_metric(\"test_recall\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "827e7ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Test Prediction on One Image \n",
    "def predict_on_image(model, image_tensor, device):\n",
    "    model.eval()\n",
    "    image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        preds = (output > 0.5).int().cpu().numpy()[0]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b52709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def create_transforms():\n",
    "    \"\"\"Create preprocessing transform pipelines for train and validation/test\"\"\"\n",
    "\n",
    "    # Training transforms with augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((178, 178)),\n",
    "        transforms.RandomCrop((160, 160)), # Removes some background and focuses on the face\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1),# Slightly vary brightness/contrast\n",
    "        transforms.ToTensor(), # Convert to tensor and scale to [0,1]\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize using ImageNet statistics\n",
    "    ])\n",
    "\n",
    "    # Validation/Test transforms without augmentation\n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.Resize((178, 178)),\n",
    "        transforms.CenterCrop((160, 160)), # Center crop (consistent, no randomness)\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    return train_transform, val_test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14bf0d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "def load_image_from_zip(image_filename, zip_file):\n",
    "    \"\"\"Load a single image from an already opened zip file\"\"\"\n",
    "    image_path_in_zip = f\"img_align_celeba/img_align_celeba/{image_filename}\"\n",
    "\n",
    "    try:\n",
    "        # Read the image data into BytesIO first to make it seekable\n",
    "        with zip_file.open(image_path_in_zip) as image_data:\n",
    "            # Read all data into memory and create a seekable BytesIO object\n",
    "            image_bytes = BytesIO(image_data.read())\n",
    "\n",
    "            # Convert to PIL Image using the seekable BytesIO object\n",
    "            image = Image.open(image_bytes)\n",
    "\n",
    "            # Convert to RGB if needed\n",
    "            if image.mode != 'RGB':\n",
    "                image = image.convert('RGB')\n",
    "\n",
    "            return image\n",
    "    except Exception as e:\n",
    "        print(f\"error loading image {image_filename}: {e}\")\n",
    "        # Return a blank image as fallback\n",
    "        return Image.new('RGB', (178, 178), color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d10f8442",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path='000067.jpg'\n",
    "zip_path='Data/img_align_celeba.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "692643ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform , val_transform=create_transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b203ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(zip_path, 'r') as zip_file:\n",
    "    image = load_image_from_zip(image_path, zip_file)  \n",
    "    image = val_transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e93259a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahhal AbuZahra\\AppData\\Local\\Temp\\ipykernel_18540\\4021340549.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(f)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.4, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=3, bias=True)\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"resnet_model.pkl\", \"rb\") as f:\n",
    "    model = torch.load(f)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8986b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_on_image(model, image, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94961dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c97cd76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Smiling': 'Not Smiling', 'Gender': 'Male', 'Hat': 'Not Wearing Hat'}\n"
     ]
    }
   ],
   "source": [
    "# 5. Map predictions to attribute names\n",
    "# attribute_names = ['Smiling','Male','Wearing Hat']\n",
    "# predicted_attributes = [attr for attr, pred in zip(attribute_names, predictions) if pred == 1]\n",
    "\n",
    "# 6. Print results\n",
    "\n",
    "print({\n",
    "    \"Smiling\": \"Smiling\" if predictions[0] == 1 else \"Not Smiling\",\n",
    "    \"Gender\": \"Male\" if predictions[1] == 1 else \"Female\",\n",
    "    \"Hat\": \"Wearing Hat\" if predictions[2] == 1 else \"Not Wearing Hat\"\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
